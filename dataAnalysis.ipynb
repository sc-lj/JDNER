{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "with open(\"data/train_500.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "entities = defaultdict(list)\n",
    "samples = []\n",
    "sample = []\n",
    "texts = []\n",
    "special_char = \"<SPACE>\"\n",
    "text_entity_pair = []\n",
    "entity_txt = ''\n",
    "entity_name = ''\n",
    "single_pair = defaultdict(list)\n",
    "for i, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        if len(sample):\n",
    "            samples.append(sample)\n",
    "            text = \"\".join([t.split(\"\\t\")[0] for t in sample])\n",
    "            text = text.replace(special_char, \" \")\n",
    "            texts.append(text)\n",
    "            if entity_name:\n",
    "                entities[entity_name].append(\"\".join(entity_txt))\n",
    "                single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "            text_entity_pair.append(\n",
    "                {\"text\": text, \"entities\": single_pair, \"sample\": sample})\n",
    "\n",
    "        entity_txt = ''\n",
    "        entity_name = \"\"\n",
    "        single_pair = defaultdict(list)\n",
    "        sample = []\n",
    "        continue\n",
    "    line = line.split(\" \")\n",
    "\n",
    "    if len(line) == 2:\n",
    "        txt, tag = line\n",
    "        sample.append(\"\\t\".join(line))\n",
    "    elif len(line) == 1:\n",
    "        tag = line[0]\n",
    "        txt = \" \"\n",
    "        sample.append(\"\\t\".join((special_char, line[0])))\n",
    "    else:\n",
    "        print(line)\n",
    "    if tag == \"O\":\n",
    "        if entity_name:\n",
    "            entities[entity_name].append(\"\".join(entity_txt))\n",
    "            single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "        entity_txt = ''\n",
    "        entity_name = \"\"\n",
    "    elif tag.startswith(\"B\"):\n",
    "        if entity_name:\n",
    "            entities[entity_name].append(\"\".join(entity_txt))\n",
    "            single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "        entity_txt = ''\n",
    "        entity_txt += txt\n",
    "        entity_name = tag.split(\"-\")[-1]\n",
    "    else:\n",
    "        entity_txt += txt\n",
    "        # entity_name = tag.split(\"-\")[-1]\n",
    "\n",
    "if len(sample):\n",
    "    text = \"\".join([t.split(\"\\t\")[0] for t in sample])\n",
    "    text = text.replace(special_char, \" \")\n",
    "    texts.append(text)\n",
    "    samples.append(sample)\n",
    "    if entity_name:\n",
    "        entities[entity_name].append(\"\".join(entity_txt))\n",
    "        single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "    text_entity_pair.append(\n",
    "        {\"text\": text, \"entities\": single_pair, \"sample\": sample})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['40', '4', '14', '5', '7', '11', '13', '8', '16', '29', '9', '12', '18', '1', '3', '22', '37', '39', '10', '36', '34', '31', '38', '54', '6', '30', '15', '2', '49', '21', '47', '23', '20', '50', '46', '41', '43', '48', '19', '52'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['。']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import pinyin,Style\n",
    "\n",
    "word = \"中\"\n",
    "pinyin(word,style=Style.TONE3,heteronym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[143, 144]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/vocust001/pretrained_models/bert_rtb3\")\n",
    "text = \"Bose SoundSport Free 真无线蓝牙耳机 运动耳机 博士防掉落耳塞 黑色\"\n",
    "# [t for t in text]\n",
    "\n",
    "tokenizer.convert_tokens_to_ids([\"a\",\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2,batch_first=True)\n",
    "input = torch.randn(30, 3, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "output, hn = rnn(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2, 20])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.transpose(1,0).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([2304, 32])\n",
      "weight_hh_l0 torch.Size([2304, 768])\n",
      "bias_ih_l0 torch.Size([2304])\n",
      "bias_hh_l0 torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(7,32)\n",
    "gru = nn.GRU(\n",
    "            32,\n",
    "            768,\n",
    "            num_layers=1,\n",
    "            batch_first=False,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "input = torch.LongTensor([[1,2,3,4],[2,3,1,5],[2,3,1,5]])\n",
    "emb = embedding(input) \n",
    "# emb.shape \n",
    "output, hn = gru(emb)\n",
    "for k,v in gru.state_dict().items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2304/768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GRU.rnn.gru_cell.candidate.weight'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import convert_tf_weight_name_to_pt_weight_name\n",
    "convert_tf_weight_name_to_pt_weight_name(\"sk_emb/GRU/rnn/gru_cell/candidate/kernel\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mnt/disk2/PythonProgram/NLPCode/PretrainModel/chinese_bert_base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.position_ids torch.Size([1, 512])\n",
      "embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig,BertModel\n",
    "model = BertModel.from_pretrained('/mnt/disk2/PythonProgram/NLPCode/PretrainModel/chinese_bert_base')\n",
    "for k,v in model.state_dict().items():\n",
    "    # if \"sk_emb\" in k and \"adam_\" not in k:\n",
    "    print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder.layer.6.attention.self.key.weight'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "layer_number = re.compile(\"_(\\d+)\")\n",
    "layer_number.sub(r\".\\1\",\"encoder.layer_6.attention.self.key.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.embeddings.word_embeddings torch.Size([768, 21128])\n",
      "bert.encoder.layer_8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.embeddings.position_embeddings torch.Size([768, 512])\n",
      "bert.encoder.layer_1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_4.intermediate.dense.weight torch.Size([3072, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.load(\"../data/plome.pt\")\n",
    "for k,v in model.items():\n",
    "    if \"adam_v\" in k or \"adam_m\" in k:\n",
    "        continue\n",
    "    # print(convert_tf_weight_name_to_pt_weight_name(k,start_prefix_to_remove=\"bert\")[0],v.shape)\n",
    "    if \"bert\" in k:\n",
    "        print(convert_tf_weight_name_to_pt_weight_name(k)[0],v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk_emb.pyemb.weight torch.Size([7, 32])\n",
      "sk_emb.GRU.weight_ih_l0 torch.Size([2304, 32])\n",
      "sk_emb.GRU.weight_hh_l0 torch.Size([2304, 768])\n",
      "sk_emb.GRU.bias_ih_l0 torch.Size([2304])\n",
      "sk_emb.GRU.bias_hh_l0 torch.Size([2304])\n",
      "py_emb.pyemb.weight torch.Size([30, 32])\n",
      "py_emb.GRU.weight_ih_l0 torch.Size([2304, 32])\n",
      "py_emb.GRU.weight_hh_l0 torch.Size([2304, 768])\n",
      "py_emb.GRU.bias_ih_l0 torch.Size([2304])\n",
      "py_emb.GRU.bias_hh_l0 torch.Size([2304])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model1 = torch.load(\"./data/new_init.pt\")\n",
    "for k,v in model1.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xiang1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import pinyin,lazy_pinyin,Style\n",
    "lazy_pinyin(\"㐮\",style=Style.TONE3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,   11,    2,    3, -100, -100, -100],\n",
       "        [-100,   11,    2,    3,    4,    5, -100]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[10,11,2,3,4,5,6],[10,11,2,3,4,5,6]])\n",
    "mask = torch.tensor([[0,1,1,1,0,0,0],[0,1,1,1,1,1,0]]).type(torch.bool)\n",
    "mask = ~mask.type(torch.bool)\n",
    "torch.masked_fill(a,mask,torch.tensor(-100))\n",
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['山本小铁子，日本女性BL漫画家与小说插画家，现居大阪。',\n",
       " '出生于1月4日，摩羯座，A型血。',\n",
       " '左撇子，喜欢鸟类，家中有两只鹦鹉，此外亦热衷于观赏棒球赛事及收集相关物品，经常将个人兴趣和生活经验融入原创的漫画作品之中。',\n",
       " '']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent_split = re.compile(\"[？？。！!]\")\n",
    "sent = \"山本小铁子，日本女性BL漫画家与小说插画家，现居大阪。出生于1月4日，摩羯座，A型血。左撇子，喜欢鸟类，家中有两只鹦鹉，此外亦热衷于观赏棒球赛事及收集相关物品，经常将个人兴趣和生活经验融入原创的漫画作品之中。\"\n",
    "sents = sent_split.split(sent)\n",
    "punct = sent_split.findall(sent)\n",
    "punct = punct+[\"\"]\n",
    "new_sents = []\n",
    "for s,p in zip(*(sents,punct)):\n",
    "    new_sents.append(s+p)\n",
    "# print(sents)\n",
    "# print(punct)\n",
    "new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC import lac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 4 {'海报', '手账本', '冰激凌机', '水粉', '内存卡', '手表', '镜头', '摄像机', '充电器', '电视', '书桌', '投影仪', '文件夹', '支架', '电脑', '投影机', '显示器', '屏幕', '手台', '钢化膜', '白板', '笔记本', '清洁套装', '黑板', '硒鼓', '画纸', '云台', '塑料盒', '台式机', '话筒', '键盘', '鼠标', '全套配件', '分杯', '游戏机', '勾线笔', '水彩', '盒子', '网卡', '手机壳', '墨水', '数码相机', '设备', '水笔', '单反相机', '手机夹', '可擦笔', '打印机', '白板笔', '礼品', '外壳', '替换芯', '挂绳', 'U盘', '闪光灯', '铅笔', '冰淇淋机', '数据线', '冰箱', '笔芯', '橡皮', '水粉颜料', '相机', '耳机', '硅胶套', '收纳袋', '写字台', '主机', 'MP3', '电子称', '马克笔'}\n",
      "40 5 {'手账'}\n",
      "40 7 {'写字台', '书桌', '外壳'}\n",
      "40 11 {'支架'}\n",
      "40 13 {'指环', '大屏幕', '礼盒装'}\n",
      "40 9 {'文件', '试卷', '自来水', '水粉画', '外壳'}\n",
      "40 12 {'水粉', '不干胶'}\n",
      "40 18 {'单反'}\n",
      "40 22 {'挂绳', '内存卡', '指环', 'U盘', '墨水', '话筒', '支架', '笔芯', '屏幕', 'u盘', '软管'}\n",
      "40 10 {'相机'}\n",
      "40 31 {'挂绳'}\n",
      "40 38 {'ipad'}\n",
      "40 54 {'单反', '笔记本'}\n",
      "4 40 {'海报', '手账本', '冰激凌机', '水粉', '内存卡', '手表', '镜头', '摄像机', '充电器', '电视', '书桌', '投影仪', '文件夹', '支架', '电脑', '投影机', '显示器', '屏幕', '手台', '钢化膜', '白板', '笔记本', '清洁套装', '黑板', '硒鼓', '画纸', '云台', '塑料盒', '台式机', '话筒', '键盘', '鼠标', '全套配件', '分杯', '游戏机', '勾线笔', '水彩', '盒子', '网卡', '手机壳', '墨水', '数码相机', '设备', '水笔', '单反相机', '手机夹', '可擦笔', '打印机', '白板笔', '礼品', '外壳', '替换芯', '挂绳', 'U盘', '闪光灯', '铅笔', '冰淇淋机', '数据线', '冰箱', '笔芯', '橡皮', '水粉颜料', '相机', '耳机', '硅胶套', '收纳袋', '写字台', '主机', 'MP3', '电子称', '马克笔'}\n",
      "4 5 {'手帐', '装饰', '视频'}\n",
      "4 7 {'写字台', '书桌', '外壳'}\n",
      "4 11 {'小钢炮', '礼物', '闹钟', '支架', '装饰'}\n",
      "4 13 {'小太阳', '整机', '手帐', '礼盒'}\n",
      "4 16 {'绿板'}\n",
      "4 9 {'资料册', '书签', '外壳'}\n",
      "4 12 {'水粉', '电暖气片'}\n",
      "4 18 {'双肩', '单机'}\n",
      "4 1 {'先科'}\n",
      "4 22 {'内存卡', '电池', '支架', '配件', '屏幕', '电源', '读卡器', '水杯', '话筒', '补光灯', '内胆', '内芯', '墨水', '万向头', '笔尖', '挂绳', '墨囊', '吸铁石', 'U盘', '礼品袋', '音箱', '充电器线', '笔芯'}\n",
      "4 10 {'相机'}\n",
      "4 31 {'挂绳'}\n",
      "4 38 {'switch'}\n",
      "4 54 {'笔记本'}\n",
      "14 5 {'DIY', '手工', '商务'}\n",
      "14 11 {'高清'}\n",
      "14 13 {'创意', '老款', '发烧级', '专业'}\n",
      "14 8 {'网红'}\n",
      "14 29 {'定做', '高清'}\n",
      "14 15 {'日本', '韩国'}\n",
      "14 47 {'高清'}\n",
      "14 50 {'经典版'}\n",
      "5 40 {'手账'}\n",
      "5 4 {'手帐', '装饰', '视频'}\n",
      "5 14 {'DIY', '手工', '商务'}\n",
      "5 7 {'家用'}\n",
      "5 11 {'书法', '录音', '手绘', '烧烤', '监控', '打印', '旋转', '批发', '记事', '商用', '分类', '收纳', '粘贴', '整理', '复印', '装饰', '运动'}\n",
      "5 13 {'抖音', '彩绘', '旋转', '快手', '手帐'}\n",
      "5 8 {'学生', '财务', '快递', '美工'}\n",
      "5 29 {'刻字'}\n",
      "5 9 {'书法', '财务', '音乐', '工程', '作业', '游戏', '素描'}\n",
      "5 10 {'广告', '王者荣耀'}\n",
      "5 38 {'吃鸡', '快手', '全民K歌'}\n",
      "5 23 {'整理'}\n",
      "5 41 {'游戏', '商用', '运动'}\n",
      "7 40 {'写字台', '书桌', '外壳'}\n",
      "7 4 {'写字台', '书桌', '外壳'}\n",
      "7 5 {'家用'}\n",
      "7 11 {'车载'}\n",
      "7 8 {'网咖'}\n",
      "7 9 {'幼儿园', '企业', '外壳', '机身'}\n",
      "7 1 {'DELL'}\n",
      "7 37 {'DELL'}\n",
      "7 43 {'办公室'}\n",
      "11 40 {'支架'}\n",
      "11 4 {'小钢炮', '礼物', '闹钟', '支架', '装饰'}\n",
      "11 14 {'高清'}\n",
      "11 5 {'书法', '录音', '手绘', '烧烤', '监控', '打印', '旋转', '批发', '记事', '商用', '分类', '收纳', '粘贴', '整理', '复印', '装饰', '运动'}\n",
      "11 7 {'车载'}\n",
      "11 13 {'电子', '全自动', '鱼眼', '外置', '充电式', '无线', '标配', '大容量', '插卡', '旋转', '折叠', '3D', '夜光', '手摇', '磁性', '有线', '通用型'}\n",
      "11 16 {'黑白'}\n",
      "11 29 {'高清', '无线', '保鲜'}\n",
      "11 9 {'书法'}\n",
      "11 18 {'标配', '4k', 'USB', '公网'}\n",
      "11 22 {'背胶', '支架'}\n",
      "11 37 {'安卓'}\n",
      "11 36 {'4G'}\n",
      "11 47 {'安卓', '高清', '全自动', '磁吸', '充电', '二合一'}\n",
      "11 23 {'整理'}\n",
      "11 41 {'商用', '运动'}\n",
      "13 40 {'指环', '大屏幕', '礼盒装'}\n",
      "13 4 {'小太阳', '整机', '手帐', '礼盒'}\n",
      "13 14 {'创意', '老款', '发烧级', '专业'}\n",
      "13 5 {'抖音', '彩绘', '旋转', '快手', '手帐'}\n",
      "13 11 {'电子', '全自动', '鱼眼', '外置', '充电式', '无线', '标配', '大容量', '插卡', '旋转', '折叠', '3D', '夜光', '手摇', '磁性', '有线', '通用型'}\n",
      "13 16 {'彩色'}\n",
      "13 29 {'无线'}\n",
      "13 9 {'插页', '英文', '数字', '活页'}\n",
      "13 12 {'皮面', '钢化', '水性', '电镀'}\n",
      "13 18 {'圆', '多机位', '多层', '双面', '大号', '标配', '单层', '微单', '双门'}\n",
      "13 22 {'黑轴', '抽屉', '指环', '按键', '拉链'}\n",
      "13 10 {'数字', '漫威'}\n",
      "13 31 {'企业级', '特细', '加长'}\n",
      "13 38 {'快手'}\n",
      "13 49 {'立式', '全屏', '同款', '曲面', '台式'}\n",
      "13 47 {'微单', '全自动'}\n",
      "8 14 {'网红'}\n",
      "8 5 {'学生', '财务', '快递', '美工'}\n",
      "8 7 {'网咖'}\n",
      "8 9 {'财务', '一年级'}\n",
      "16 4 {'绿板'}\n",
      "16 11 {'黑白'}\n",
      "16 13 {'彩色'}\n",
      "16 9 {'色彩'}\n",
      "16 34 {'蓝色', '金色', '红色'}\n",
      "16 6 {'土豪金'}\n",
      "29 14 {'定做', '高清'}\n",
      "29 5 {'刻字'}\n",
      "29 11 {'高清', '无线', '保鲜'}\n",
      "29 13 {'无线'}\n",
      "29 22 {'可定制'}\n",
      "29 47 {'高清'}\n",
      "9 40 {'文件', '试卷', '自来水', '水粉画', '外壳'}\n",
      "9 4 {'资料册', '书签', '外壳'}\n",
      "9 5 {'书法', '财务', '音乐', '工程', '作业', '游戏', '素描'}\n",
      "9 7 {'幼儿园', '企业', '外壳', '机身'}\n",
      "9 11 {'书法'}\n",
      "9 13 {'插页', '英文', '数字', '活页'}\n",
      "9 8 {'财务', '一年级'}\n",
      "9 16 {'色彩'}\n",
      "9 37 {'申通'}\n",
      "9 10 {'标签', '英语', '语文', '数字'}\n",
      "9 41 {'游戏'}\n",
      "12 40 {'水粉', '不干胶'}\n",
      "12 4 {'水粉', '电暖气片'}\n",
      "12 13 {'皮面', '钢化', '水性', '电镀'}\n",
      "12 30 {'液晶', '金属', '不锈钢', '玻璃'}\n",
      "18 40 {'单反'}\n",
      "18 4 {'双肩', '单机'}\n",
      "18 11 {'标配', '4k', 'USB', '公网'}\n",
      "18 13 {'圆', '多机位', '多层', '双面', '大号', '标配', '单层', '微单', '双门'}\n",
      "18 3 {'B5'}\n",
      "18 36 {'30个', '2G', '16G', 'U段', '14英寸', '四核', '32G', '120G'}\n",
      "18 38 {'A3', '8', 'A5'}\n",
      "18 54 {'10支', '5个装', '单反', 'a4', '100支', 'A4', '1支', '14英寸', '12英寸', '5.8英寸'}\n",
      "18 47 {'3d', '微单'}\n",
      "1 4 {'先科'}\n",
      "1 7 {'DELL'}\n",
      "1 37 {'联想', 'HUAWEI', '佳能', '京瓷', '华硕', '曼富图', '戴尔', '美的', '华为', 'DELL', '电信', '索尼', '松下'}\n",
      "1 2 {'樱花'}\n",
      "1 19 {'英特尔'}\n",
      "3 18 {'B5'}\n",
      "3 38 {'苹果XS', 'mate20', 'P30', 'XR'}\n",
      "22 40 {'挂绳', '内存卡', '指环', 'U盘', '墨水', '话筒', '支架', '笔芯', '屏幕', 'u盘', '软管'}\n",
      "22 4 {'内存卡', '电池', '支架', '配件', '屏幕', '电源', '读卡器', '水杯', '话筒', '补光灯', '内胆', '内芯', '墨水', '万向头', '笔尖', '挂绳', '墨囊', '吸铁石', 'U盘', '礼品袋', '音箱', '充电器线', '笔芯'}\n",
      "22 11 {'背胶', '支架'}\n",
      "22 13 {'黑轴', '抽屉', '指环', '按键', '拉链'}\n",
      "22 29 {'可定制'}\n",
      "22 31 {'挂绳', '大屏'}\n",
      "37 7 {'DELL'}\n",
      "37 11 {'安卓'}\n",
      "37 9 {'申通'}\n",
      "37 1 {'联想', 'HUAWEI', '佳能', '京瓷', '华硕', '曼富图', '戴尔', '美的', '华为', 'DELL', '电信', '索尼', '松下'}\n",
      "37 38 {'荣耀'}\n",
      "37 47 {'安卓', 'iPhone', '苹果'}\n",
      "10 40 {'相机'}\n",
      "10 4 {'相机'}\n",
      "10 5 {'广告', '王者荣耀'}\n",
      "10 13 {'数字', '漫威'}\n",
      "10 9 {'标签', '英语', '语文', '数字'}\n",
      "36 11 {'4G'}\n",
      "36 18 {'30个', '2G', '16G', 'U段', '14英寸', '四核', '32G', '120G'}\n",
      "36 54 {'14英寸'}\n",
      "36 30 {'油性'}\n",
      "34 16 {'蓝色', '金色', '红色'}\n",
      "31 40 {'挂绳'}\n",
      "31 4 {'挂绳'}\n",
      "31 13 {'企业级', '特细', '加长'}\n",
      "31 22 {'挂绳', '大屏'}\n",
      "38 40 {'ipad'}\n",
      "38 4 {'switch'}\n",
      "38 5 {'吃鸡', '快手', '全民K歌'}\n",
      "38 13 {'快手'}\n",
      "38 18 {'A3', '8', 'A5'}\n",
      "38 3 {'苹果XS', 'mate20', 'P30', 'XR'}\n",
      "38 37 {'荣耀'}\n",
      "38 54 {'11'}\n",
      "38 50 {'青春版'}\n",
      "54 40 {'单反', '笔记本'}\n",
      "54 4 {'笔记本'}\n",
      "54 18 {'10支', '5个装', '单反', 'a4', '100支', 'A4', '1支', '14英寸', '12英寸', '5.8英寸'}\n",
      "54 36 {'14英寸'}\n",
      "54 38 {'11'}\n",
      "6 16 {'土豪金'}\n",
      "30 12 {'液晶', '金属', '不锈钢', '玻璃'}\n",
      "30 36 {'油性'}\n",
      "15 14 {'日本', '韩国'}\n",
      "2 1 {'樱花'}\n",
      "49 13 {'立式', '全屏', '同款', '曲面', '台式'}\n",
      "49 48 {'软质'}\n",
      "47 14 {'高清'}\n",
      "47 11 {'安卓', '高清', '全自动', '磁吸', '充电', '二合一'}\n",
      "47 13 {'微单', '全自动'}\n",
      "47 29 {'高清'}\n",
      "47 18 {'3d', '微单'}\n",
      "47 37 {'安卓', 'iPhone', '苹果'}\n",
      "47 41 {'导航'}\n",
      "23 5 {'整理'}\n",
      "23 11 {'整理'}\n",
      "50 14 {'经典版'}\n",
      "50 38 {'青春版'}\n",
      "41 5 {'游戏', '商用', '运动'}\n",
      "41 11 {'商用', '运动'}\n",
      "41 9 {'游戏'}\n",
      "41 47 {'导航'}\n",
      "43 7 {'办公室'}\n",
      "48 49 {'软质'}\n",
      "19 1 {'英特尔'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/entites.json\",'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for k,v in data.items():\n",
    "    for k1,v1 in data.items():\n",
    "        if k == k1:\n",
    "            continue\n",
    "        inter = set(v).intersection(set(v1))\n",
    "        if len(inter)>0:\n",
    "            print(k,k1,inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC.lac import LAC\n",
    "lac = LAC()\n",
    "a = lac.run([\"智比奈特（ZBNET）网卡ZBE9602EF千兆双口多模SFP光纤网卡intel82576光纤网卡\",\"智比奈特（ZBNET）网卡ZBE9602EF千兆双口多模SFP光纤网卡intel82576光纤网卡\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['智比奈特', '（', 'ZBNET', '）', '网卡', 'ZBE9602EF', '千兆双口', '多模SFP', '光纤', '网卡', 'intel82576光纤', '网卡'] ['nz', 'w', 'n', 'w', 'n', 'n', 'nz', 'nz', 'n', 'n', 'n', 'n']\n",
      "['智比奈特', '（', 'ZBNET', '）', '网卡', 'ZBE9602EF', '千兆双口', '多模SFP', '光纤', '网卡', 'intel82576光纤', '网卡'] ['nz', 'w', 'n', 'w', 'n', 'n', 'nz', 'nz', 'n', 'n', 'n', 'n']\n"
     ]
    }
   ],
   "source": [
    "for cut_text,cut_seg in a:\n",
    "    print(cut_text,cut_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['麦克风', '对讲机', '话筒']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_ent = synonyms.nearby(\"扩音器\")\n",
    "[word for word, pro in zip(*choice_ent) if 1 > pro > 0.65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC.lac import LAC"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f97b5404fb095f095ec6d82ad5ea760d3c89ebf48380ba446a09f5481b5cab8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
