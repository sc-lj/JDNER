{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "with open(\"data/train_500.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "entities = defaultdict(list)\n",
    "samples = []\n",
    "sample = []\n",
    "texts = []\n",
    "special_char = \"<SPACE>\"\n",
    "text_entity_pair = []\n",
    "entity_txt = ''\n",
    "entity_name = ''\n",
    "single_pair = defaultdict(list)\n",
    "for i, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        if len(sample):\n",
    "            samples.append(sample)\n",
    "            text = \"\".join([t.split(\"\\t\")[0] for t in sample])\n",
    "            text = text.replace(special_char, \" \")\n",
    "            texts.append(text)\n",
    "            if entity_name:\n",
    "                entities[entity_name].append(\"\".join(entity_txt))\n",
    "                single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "            text_entity_pair.append(\n",
    "                {\"text\": text, \"entities\": single_pair, \"sample\": sample})\n",
    "\n",
    "        entity_txt = ''\n",
    "        entity_name = \"\"\n",
    "        single_pair = defaultdict(list)\n",
    "        sample = []\n",
    "        continue\n",
    "    line = line.split(\" \")\n",
    "\n",
    "    if len(line) == 2:\n",
    "        txt, tag = line\n",
    "        sample.append(\"\\t\".join(line))\n",
    "    elif len(line) == 1:\n",
    "        tag = line[0]\n",
    "        txt = \" \"\n",
    "        sample.append(\"\\t\".join((special_char, line[0])))\n",
    "    else:\n",
    "        print(line)\n",
    "    if tag == \"O\":\n",
    "        if entity_name:\n",
    "            entities[entity_name].append(\"\".join(entity_txt))\n",
    "            single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "        entity_txt = ''\n",
    "        entity_name = \"\"\n",
    "    elif tag.startswith(\"B\"):\n",
    "        if entity_name:\n",
    "            entities[entity_name].append(\"\".join(entity_txt))\n",
    "            single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "        entity_txt = ''\n",
    "        entity_txt += txt\n",
    "        entity_name = tag.split(\"-\")[-1]\n",
    "    else:\n",
    "        entity_txt += txt\n",
    "        # entity_name = tag.split(\"-\")[-1]\n",
    "\n",
    "if len(sample):\n",
    "    text = \"\".join([t.split(\"\\t\")[0] for t in sample])\n",
    "    text = text.replace(special_char, \" \")\n",
    "    texts.append(text)\n",
    "    samples.append(sample)\n",
    "    if entity_name:\n",
    "        entities[entity_name].append(\"\".join(entity_txt))\n",
    "        single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "    text_entity_pair.append(\n",
    "        {\"text\": text, \"entities\": single_pair, \"sample\": sample})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['40', '4', '14', '5', '7', '11', '13', '8', '16', '29', '9', '12', '18', '1', '3', '22', '37', '39', '10', '36', '34', '31', '38', '54', '6', '30', '15', '2', '49', '21', '47', '23', '20', '50', '46', '41', '43', '48', '19', '52'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['。']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import pinyin,Style\n",
    "\n",
    "word = \"中\"\n",
    "pinyin(word,style=Style.TONE3,heteronym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[143, 144]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/vocust001/pretrained_models/bert_rtb3\")\n",
    "text = \"Bose SoundSport Free 真无线蓝牙耳机 运动耳机 博士防掉落耳塞 黑色\"\n",
    "# [t for t in text]\n",
    "\n",
    "tokenizer.convert_tokens_to_ids([\"a\",\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2,batch_first=True)\n",
    "input = torch.randn(30, 3, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "output, hn = rnn(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2, 20])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.transpose(1,0).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([2304, 32])\n",
      "weight_hh_l0 torch.Size([2304, 768])\n",
      "bias_ih_l0 torch.Size([2304])\n",
      "bias_hh_l0 torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(7,32)\n",
    "gru = nn.GRU(\n",
    "            32,\n",
    "            768,\n",
    "            num_layers=1,\n",
    "            batch_first=False,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "input = torch.LongTensor([[1,2,3,4],[2,3,1,5],[2,3,1,5]])\n",
    "emb = embedding(input) \n",
    "# emb.shape \n",
    "output, hn = gru(emb)\n",
    "for k,v in gru.state_dict().items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2304/768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GRU.rnn.gru_cell.candidate.weight'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import convert_tf_weight_name_to_pt_weight_name\n",
    "convert_tf_weight_name_to_pt_weight_name(\"sk_emb/GRU/rnn/gru_cell/candidate/kernel\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mnt/disk2/PythonProgram/NLPCode/PretrainModel/chinese_bert_base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.position_ids torch.Size([1, 512])\n",
      "embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig,BertModel\n",
    "model = BertModel.from_pretrained('/mnt/disk2/PythonProgram/NLPCode/PretrainModel/chinese_bert_base')\n",
    "for k,v in model.state_dict().items():\n",
    "    # if \"sk_emb\" in k and \"adam_\" not in k:\n",
    "    print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder.layer.6.attention.self.key.weight'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "layer_number = re.compile(\"_(\\d+)\")\n",
    "layer_number.sub(r\".\\1\",\"encoder.layer_6.attention.self.key.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.embeddings.word_embeddings torch.Size([768, 21128])\n",
      "bert.encoder.layer_8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.embeddings.position_embeddings torch.Size([768, 512])\n",
      "bert.encoder.layer_1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer_7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer_3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer_3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer_8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer_6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer_4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer_11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer_4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer_4.intermediate.dense.weight torch.Size([3072, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.load(\"../data/plome.pt\")\n",
    "for k,v in model.items():\n",
    "    if \"adam_v\" in k or \"adam_m\" in k:\n",
    "        continue\n",
    "    # print(convert_tf_weight_name_to_pt_weight_name(k,start_prefix_to_remove=\"bert\")[0],v.shape)\n",
    "    if \"bert\" in k:\n",
    "        print(convert_tf_weight_name_to_pt_weight_name(k)[0],v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py_emb.pyemb.weight torch.Size([30, 32])\n",
      "py_emb.GRU.weight_ih_l0 torch.Size([2304, 32])\n",
      "py_emb.GRU.weight_hh_l0 torch.Size([2304, 768])\n",
      "py_emb.GRU.bias_ih_l0 torch.Size([2304])\n",
      "py_emb.GRU.bias_hh_l0 torch.Size([2304])\n",
      "sk_emb.pyemb.weight torch.Size([7, 32])\n",
      "sk_emb.GRU.weight_ih_l0 torch.Size([2304, 32])\n",
      "sk_emb.GRU.weight_hh_l0 torch.Size([2304, 768])\n",
      "sk_emb.GRU.bias_ih_l0 torch.Size([2304])\n",
      "sk_emb.GRU.bias_hh_l0 torch.Size([2304])\n",
      "embeddings.position_ids torch.Size([1, 512])\n",
      "embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "cls.weight torch.Size([81, 768])\n",
      "cls.bias torch.Size([81])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model1 = torch.load(\"../data/init.pt\")\n",
    "for k,v in model1.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xiang1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import pinyin,lazy_pinyin,Style\n",
    "lazy_pinyin(\"㐮\",style=Style.TONE3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,   11,    2,    3, -100, -100, -100],\n",
       "        [-100,   11,    2,    3,    4,    5, -100]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[10,11,2,3,4,5,6],[10,11,2,3,4,5,6]])\n",
    "mask = torch.tensor([[0,1,1,1,0,0,0],[0,1,1,1,1,1,0]]).type(torch.bool)\n",
    "mask = ~mask.type(torch.bool)\n",
    "torch.masked_fill(a,mask,torch.tensor(-100))\n",
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"山本小铁子\\n\\n山本小铁子，日本女性BL漫画家与小说插画家，现居大阪。出生于1月4日，摩羯座，A型血。左撇子，喜欢鸟类，家中有两只鹦鹉，此外亦热衷于观赏棒球赛事及收集相关物品，经常将个人兴趣和生活经验融入原创的漫画作品之中。\\n\\n笔名来自已故的日本职业摔角选手，又因为自己是女生的关系，故在后面加上一个「子」字。\\n\\n目前主要从事创作连载漫画、绘制小说插图与协助小说漫画化等工作，作品经常刊登于幻冬舍、、海王社、白泉社等日本知名出版社的杂志之中。\\n\\n原创作品风格温馨自然，内容有趣而诙谐，取材贴近生活，结局喜剧为主。不过做为BL漫画家，其作品的性爱场面明显偏少，用较多篇幅描绘主角间的暧昧情愫。\\n\\n起初是以少女漫画家的身分出道，在1995、1996年期间曾以「」为名，在白泉社杂志《花与梦》发表〈〉、〈〉、〈〉、〈〉、〈〉、〈〉等漫画作品，但过程并不顺利，以致于均未进一步集结成单行本。此外亦曾以「」为笔名。\\n\\n后来经朋友影响，接触到同人志贩售会与相关作品，转而成为同人志作家，也开始从事跟BL相关的二次创作，直至今日仍以「」与「」为社团名称参与同人活动，其作画的内容大多是参考日本偶像团体「Kinki Kids」以及动画「新机动战记钢弹W」中的角色衍生而来，近年则开始投入漫画《影子篮球员》的同人创作，并主打「黄黑」配对，现已推出《AS YOU LIKE IT》、《》、《first xxx》、《》、《I love you》、《Facta ,non verba》、《》等作品。\\n\\n2001年，受业界编辑邀约，开始尝试创作并发行BL向的连载漫画与单行本，持续至今。本身是业界少见的高产量作家，每个月平均要更新四部作品，完成一百页以上的创作，目前手上有九部作品正在连载，主要负责故事发想以及原画、上线的部分，其他部分是由助手田中小姐协助。\\n\\n过去除了同人志活动外很少公开露面，自2011年起，才开始配合部分单行本之新刊出版，在日本各地举办个人签名会。2013年8月15日，更接受台湾东贩之邀请来台湾（前一天已先到台湾），于第14届漫画博览会开幕当天举办签名会，作品《野蛮情人》第六册中文版亦同日发行，这是小铁子个人生涯第一次造访台湾。\\n\\n套色者为连载中、未完结的漫画。\\n\\n依他人之原著小说内容，漫画化后的非原创作品，剧情不限于BL题材。\\n\\n已有部分或全部单行本化的原创作品，均为BL向。\\n\\n在杂志上持续连载，尚未单行本化的原创作品。\\n\\n在杂志上曾经出现，但刊载一话后未持续连载，亦未单行本化的原创作品，均为BL向。\\n\\n做为相关纪念活动的赠品，剧情多为原创作品之番外，均为BL向。\\n\\n原创作品有声化后的专辑，均为BL向。\\n\\n于他人之原著小说作品中，协助封面、封底与内页部分之插画。\\n\\n一般向作品\\n\\n\\nBL向作品\\n\\n\\n早年除了同人志贩售场合外，很少公开露面，自2011年起，商业志签名会次数才明显增加。\\n\\n\\n\\n以下资讯多来自漫画后记中的自述。\\n\\n\\n出版社\\n\\n编辑部网志\\n\\n线上免费阅览（正版）\\n\"\n",
    "import re\n",
    "sent_split = re.compile(\"[？？。！!]\")\n",
    "\n",
    "for sent in text.strip().split(\"\\n\"):\n",
    "    sents = sent_split.split(sent)\n",
    "    punct = sent_split.findall(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['山本小铁子，日本女性BL漫画家与小说插画家，现居大阪。',\n",
       " '出生于1月4日，摩羯座，A型血。',\n",
       " '左撇子，喜欢鸟类，家中有两只鹦鹉，此外亦热衷于观赏棒球赛事及收集相关物品，经常将个人兴趣和生活经验融入原创的漫画作品之中。',\n",
       " '']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"山本小铁子，日本女性BL漫画家与小说插画家，现居大阪。出生于1月4日，摩羯座，A型血。左撇子，喜欢鸟类，家中有两只鹦鹉，此外亦热衷于观赏棒球赛事及收集相关物品，经常将个人兴趣和生活经验融入原创的漫画作品之中。\"\n",
    "sents = sent_split.split(sent)\n",
    "punct = sent_split.findall(sent)\n",
    "punct = punct+[\"\"]\n",
    "new_sents = []\n",
    "for s,p in zip(*(sents,punct)):\n",
    "    new_sents.append(s+p)\n",
    "# print(sents)\n",
    "# print(punct)\n",
    "new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f97b5404fb095f095ec6d82ad5ea760d3c89ebf48380ba446a09f5481b5cab8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
