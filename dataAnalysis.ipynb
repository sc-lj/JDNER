{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "with open(\"data/train_500.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "entities = defaultdict(list)\n",
    "samples = []\n",
    "sample = []\n",
    "texts = []\n",
    "special_char = \"<SPACE>\"\n",
    "text_entity_pair = []\n",
    "entity_txt = ''\n",
    "entity_name = ''\n",
    "single_pair = defaultdict(list)\n",
    "for i, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        if len(sample):\n",
    "            samples.append(sample)\n",
    "            text = \"\".join([t.split(\"\\t\")[0] for t in sample])\n",
    "            text = text.replace(special_char, \" \")\n",
    "            texts.append(text)\n",
    "            if entity_name:\n",
    "                entities[entity_name].append(\"\".join(entity_txt))\n",
    "                single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "            text_entity_pair.append(\n",
    "                {\"text\": text, \"entities\": single_pair, \"sample\": sample})\n",
    "\n",
    "        entity_txt = ''\n",
    "        entity_name = \"\"\n",
    "        single_pair = defaultdict(list)\n",
    "        sample = []\n",
    "        continue\n",
    "    line = line.split(\" \")\n",
    "\n",
    "    if len(line) == 2:\n",
    "        txt, tag = line\n",
    "        sample.append(\"\\t\".join(line))\n",
    "    elif len(line) == 1:\n",
    "        tag = line[0]\n",
    "        txt = \" \"\n",
    "        sample.append(\"\\t\".join((special_char, line[0])))\n",
    "    else:\n",
    "        print(line)\n",
    "    if tag == \"O\":\n",
    "        if entity_name:\n",
    "            entities[entity_name].append(\"\".join(entity_txt))\n",
    "            single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "        entity_txt = ''\n",
    "        entity_name = \"\"\n",
    "    elif tag.startswith(\"B\"):\n",
    "        if entity_name:\n",
    "            entities[entity_name].append(\"\".join(entity_txt))\n",
    "            single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "        entity_txt = ''\n",
    "        entity_txt += txt\n",
    "        entity_name = tag.split(\"-\")[-1]\n",
    "    else:\n",
    "        entity_txt += txt\n",
    "        # entity_name = tag.split(\"-\")[-1]\n",
    "\n",
    "if len(sample):\n",
    "    text = \"\".join([t.split(\"\\t\")[0] for t in sample])\n",
    "    text = text.replace(special_char, \" \")\n",
    "    texts.append(text)\n",
    "    samples.append(sample)\n",
    "    if entity_name:\n",
    "        entities[entity_name].append(\"\".join(entity_txt))\n",
    "        single_pair[entity_name].append(\"\".join(entity_txt))\n",
    "    text_entity_pair.append(\n",
    "        {\"text\": text, \"entities\": single_pair, \"sample\": sample})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import pinyin,Style\n",
    "\n",
    "word = \"中\"\n",
    "pinyin(word,style=Style.TONE3,heteronym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/vocust001/pretrained_models/bert_rtb3\")\n",
    "text = \"Bose SoundSport Free 真无线蓝牙耳机 运动耳机 博士防掉落耳塞 黑色\"\n",
    "# [t for t in text]\n",
    "\n",
    "tokenizer.convert_tokens_to_ids([\"a\",\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2,batch_first=True)\n",
    "input = torch.randn(30, 3, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "output, hn = rnn(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn.transpose(1,0).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(7,32)\n",
    "gru = nn.GRU(\n",
    "            32,\n",
    "            768,\n",
    "            num_layers=1,\n",
    "            batch_first=False,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "input = torch.LongTensor([[1,2,3,4],[2,3,1,5],[2,3,1,5]])\n",
    "emb = embedding(input) \n",
    "# emb.shape \n",
    "output, hn = gru(emb)\n",
    "for k,v in gru.state_dict().items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2304/768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import convert_tf_weight_name_to_pt_weight_name\n",
    "convert_tf_weight_name_to_pt_weight_name(\"sk_emb/GRU/rnn/gru_cell/candidate/kernel\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig,BertModel\n",
    "model = BertModel.from_pretrained('/mnt/disk2/PythonProgram/NLPCode/PretrainModel/chinese_bert_base')\n",
    "for k,v in model.state_dict().items():\n",
    "    # if \"sk_emb\" in k and \"adam_\" not in k:\n",
    "    print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "layer_number = re.compile(\"_(\\d+)\")\n",
    "layer_number.sub(r\".\\1\",\"encoder.layer_6.attention.self.key.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"../data/plome.pt\")\n",
    "for k,v in model.items():\n",
    "    if \"adam_v\" in k or \"adam_m\" in k:\n",
    "        continue\n",
    "    # print(convert_tf_weight_name_to_pt_weight_name(k,start_prefix_to_remove=\"bert\")[0],v.shape)\n",
    "    if \"bert\" in k:\n",
    "        print(convert_tf_weight_name_to_pt_weight_name(k)[0],v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model1 = torch.load(\"./data/new_init.pt\")\n",
    "for k,v in model1.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import pinyin,lazy_pinyin,Style\n",
    "lazy_pinyin(\"㐮\",style=Style.TONE3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[10,11,2,3,4,5,6],[10,11,2,3,4,5,6]])\n",
    "mask = torch.tensor([[0,1,1,1,0,0,0],[0,1,1,1,1,1,0]]).type(torch.bool)\n",
    "mask = ~mask.type(torch.bool)\n",
    "torch.masked_fill(a,mask,torch.tensor(-100))\n",
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sent_split = re.compile(\"[？？。！!]\")\n",
    "sent = \"山本小铁子，日本女性BL漫画家与小说插画家，现居大阪。出生于1月4日，摩羯座，A型血。左撇子，喜欢鸟类，家中有两只鹦鹉，此外亦热衷于观赏棒球赛事及收集相关物品，经常将个人兴趣和生活经验融入原创的漫画作品之中。\"\n",
    "sents = sent_split.split(sent)\n",
    "punct = sent_split.findall(sent)\n",
    "punct = punct+[\"\"]\n",
    "new_sents = []\n",
    "for s,p in zip(*(sents,punct)):\n",
    "    new_sents.append(s+p)\n",
    "# print(sents)\n",
    "# print(punct)\n",
    "new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC import lac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/entites.json\",'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for k,v in data.items():\n",
    "    for k1,v1 in data.items():\n",
    "        if k == k1:\n",
    "            continue\n",
    "        inter = set(v).intersection(set(v1))\n",
    "        if len(inter)>0:\n",
    "            print(k,k1,inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC.lac import LAC\n",
    "lac = LAC()\n",
    "a = lac.run([\"智比奈特（ZBNET）网卡ZBE9602EF千兆双口多模SFP光纤网卡intel82576光纤网卡\",\"智比奈特（ZBNET）网卡ZBE9602EF千兆双口多模SFP光纤网卡intel82576光纤网卡\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cut_text,cut_seg in a:\n",
    "    print(cut_text,cut_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_ent = synonyms.nearby(\"保护贴膜\")\n",
    "[word for word, pro in zip(*choice_ent) if 1 > pro > 0.65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC.lac import LAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "pointer_label = np.zeros((10,50,50))\n",
    "num = 10\n",
    "length = 50\n",
    "max_length = 60\n",
    "pad_1 = np.zeros((num, length, (max_length-length)))\n",
    "pad_2 = np.zeros((num, (max_length-length), max_length))\n",
    "pointer_label = np.concatenate([pointer_label, pad_1], axis=2)\n",
    "pointer_label = np.concatenate([pointer_label, pad_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "kewords = KeywordProcessor()\n",
    "kewords.non_word_boundaries =set()\n",
    "kewords.add_keywords_from_list([\n",
    "                \"8plus\",\n",
    "                \"iphone8\",\n",
    "                \"7plus\",\n",
    "                \"iPhone 7p/8p\"\n",
    "            ])\n",
    "\n",
    "kewords.extract_keywords(\"豪邦 苹果8plus手机壳网红iphone8个性超薄玻璃7plus保护套7全包防摔硬壳潮牌女款 口吻生花+送钢化膜 iphone 7p/8p\",span_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/2022京东电商数据比赛/京东商品标题实体识别数据集/preliminary_test_a/word_per_line_preliminary_A.txt\",'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"data/2022京东电商数据比赛/京东商品标题实体识别数据集/preliminary_test_a/crf_predict_v1.txt\",'r') as f:\n",
    "    lines2 = f.readlines()\n",
    "\n",
    "number = len(lines2)\n",
    "for i in range(number):\n",
    "    l2 = lines2[i].strip(\"\\n\")\n",
    "    l2 = l2.split(\" \")[0]\n",
    "    l1 = lines[i].strip()\n",
    "    if l2.lower()!=l1.lower():\n",
    "        print(i,l2.lower(),l1.lower())\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的: 595795\n",
      "标签:  1 数量:22587\n",
      "标签: 10 数量: 7434\n",
      "标签: 11 数量:54234\n",
      "标签: 12 数量:10972\n",
      "标签: 13 数量:58124\n",
      "标签: 14 数量:20169\n",
      "标签: 15 数量:  735\n",
      "标签: 16 数量:20599\n",
      "标签: 17 数量:   25\n",
      "标签: 18 数量:48894\n",
      "标签: 19 数量:  111\n",
      "标签:  2 数量: 2780\n",
      "标签: 20 数量:  532\n",
      "标签: 21 数量:  558\n",
      "标签: 22 数量: 8477\n",
      "标签: 23 数量:   19\n",
      "标签: 24 数量:    5\n",
      "标签: 25 数量:   22\n",
      "标签: 26 数量:    1\n",
      "标签: 28 数量:   29\n",
      "标签: 29 数量: 3894\n",
      "标签:  3 数量: 8201\n",
      "标签: 30 数量:  484\n",
      "标签: 31 数量:  777\n",
      "标签: 32 数量:   42\n",
      "标签: 33 数量:   13\n",
      "标签: 34 数量:  241\n",
      "标签: 35 数量:    2\n",
      "标签: 36 数量: 3336\n",
      "标签: 37 数量:13574\n",
      "标签: 38 数量:27734\n",
      "标签: 39 数量: 4383\n",
      "标签:  4 数量:150320\n",
      "标签: 40 数量:29217\n",
      "标签: 41 数量:  446\n",
      "标签: 42 数量:    7\n",
      "标签: 43 数量:   73\n",
      "标签: 44 数量:   33\n",
      "标签: 46 数量:   20\n",
      "标签: 47 数量: 1204\n",
      "标签: 48 数量:  153\n",
      "标签: 49 数量: 1239\n",
      "标签:  5 数量:36385\n",
      "标签: 50 数量:  346\n",
      "标签: 51 数量:   14\n",
      "标签: 52 数量:  162\n",
      "标签: 53 数量:    5\n",
      "标签: 54 数量: 5398\n",
      "标签:  6 数量: 1354\n",
      "标签:  7 数量:22388\n",
      "标签:  8 数量:16431\n",
      "标签:  9 数量:11612\n"
     ]
    }
   ],
   "source": [
    "def build_entity(tags):\n",
    "    \"\"\"构建实体span\n",
    "\n",
    "    Args:\n",
    "        tags (_type_): _description_\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    tmp_ent = \"\"\n",
    "    start = 0\n",
    "    for index, tag in enumerate(tags):\n",
    "        if tag.startswith(\"B\"):\n",
    "            if tmp_ent:\n",
    "                entities.append((tmp_ent, start, index))\n",
    "                tmp_ent = \"\"\n",
    "            start = index\n",
    "            tmp_ent = tag[2:]\n",
    "        elif tag.startswith(\"O\"):\n",
    "            if tmp_ent:\n",
    "                entities.append((tmp_ent, start, index))\n",
    "                tmp_ent = \"\"\n",
    "            start = index\n",
    "    if tmp_ent:\n",
    "        entities.append((tmp_ent, start, index))\n",
    "        tmp_ent = \"\"\n",
    "    return entities\n",
    "import json\n",
    "from collections import defaultdict\n",
    "def static_entity_info(filenames):\n",
    "    with open(filenames,'r') as f:\n",
    "        lines = json.load(f)\n",
    "    \n",
    "    entities = defaultdict(int)\n",
    "    for line in lines:\n",
    "        sample = line['sample']\n",
    "        tags = [l.split(\"\\t\")[-1] for l in sample]\n",
    "        entity = build_entity(tags)\n",
    "        for ent,_,_ in entity:\n",
    "            entities[ent]+=1\n",
    "    \n",
    "    print(\"总的:\",sum(entities.values()))\n",
    "    entities = sorted(entities.items(),key=lambda x:x[0])\n",
    "    for lab,values in entities:\n",
    "        print(\"标签:%3s 数量:%5d\"%(lab,values))\n",
    "    \n",
    "\n",
    "# static_entity_info(\"data/val_corrected.json\")\n",
    "static_entity_info(\"data/train_corrected.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的: 596512\n",
      "标签:  1 数量:22600\n",
      "标签: 10 数量: 7441\n",
      "标签: 11 数量:54322\n",
      "标签: 12 数量:10982\n",
      "标签: 13 数量:58197\n",
      "标签: 14 数量:20183\n",
      "标签: 15 数量:  735\n",
      "标签: 16 数量:20620\n",
      "标签: 17 数量:   25\n",
      "标签: 18 数量:48976\n",
      "标签: 19 数量:  111\n",
      "标签:  2 数量: 2780\n",
      "标签: 20 数量:  532\n",
      "标签: 21 数量:  558\n",
      "标签: 22 数量: 8487\n",
      "标签: 23 数量:   19\n",
      "标签: 24 数量:    5\n",
      "标签: 25 数量:   22\n",
      "标签: 26 数量:    1\n",
      "标签: 28 数量:   29\n",
      "标签: 29 数量: 3899\n",
      "标签:  3 数量: 8207\n",
      "标签: 30 数量:  486\n",
      "标签: 31 数量:  778\n",
      "标签: 32 数量:   42\n",
      "标签: 33 数量:   13\n",
      "标签: 34 数量:  241\n",
      "标签: 35 数量:    2\n",
      "标签: 36 数量: 3337\n",
      "标签: 37 数量:13585\n",
      "标签: 38 数量:27747\n",
      "标签: 39 数量: 4384\n",
      "标签:  4 数量:150513\n",
      "标签: 40 数量:29248\n",
      "标签: 41 数量:  446\n",
      "标签: 42 数量:    7\n",
      "标签: 43 数量:   73\n",
      "标签: 44 数量:   33\n",
      "标签: 46 数量:   20\n",
      "标签: 47 数量: 1204\n",
      "标签: 48 数量:  153\n",
      "标签: 49 数量: 1239\n",
      "标签:  5 数量:36446\n",
      "标签: 50 数量:  346\n",
      "标签: 51 数量:   14\n",
      "标签: 52 数量:  162\n",
      "标签: 53 数量:    5\n",
      "标签: 54 数量: 5399\n",
      "标签:  6 数量: 1354\n",
      "标签:  7 数量:22427\n",
      "标签:  8 数量:16453\n",
      "标签:  9 数量:11624\n"
     ]
    }
   ],
   "source": [
    "# static_entity_info(\"data/val.json\")\n",
    "static_entity_info(\"data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f97b5404fb095f095ec6d82ad5ea760d3c89ebf48380ba446a09f5481b5cab8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
